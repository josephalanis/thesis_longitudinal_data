{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iR4Le9sWSP_4Vikn_IZmerKfDndp_liv",
      "authorship_tag": "ABX9TyN1tCRhZ7nDSEyR8J6XjJ3c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephalanis/thesis_longitudinal_data/blob/main/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTx2mJvSrTbE",
        "outputId": "25708aa4-f103-4d81-ce60-45699d0ffa31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyadic in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyadic) (1.25.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from pyadic) (1.12.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->pyadic) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.models import Model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from functools import partial\n",
        "import math as math\n",
        "from fractions import Fraction\n",
        "from sklearn.metrics import DistanceMetric\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "# Creating code to handle Missing data\n",
        "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "\n",
        "#Converting a fraction into a p-adic number\n",
        "%pip install pyadic\n",
        "from pyadic import PAdic, ModP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your dataset and Info.txt file\n",
        "\n",
        "path =\"/content/drive/MyDrive/Colab/Thesis/\"\n",
        "info_file =\"/content/drive/MyDrive/Colab/Thesis/Miss.csv\"\n",
        "info_file1=\"/content/drive/MyDrive/Colab/Thesis/dtTrail3.csv\"\n",
        "info_file2=\"/content/drive/MyDrive/Colab/Thesis/s2.csv\"\n",
        "\n",
        "Miss = pd.read_csv(info_file)\n",
        "Original=pd.read_csv(info_file1)\n",
        "sample=pd.read_csv(info_file2)\n",
        "#info = info.drop('Unnamed: 7', axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "ggYHIavpsRhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Viewing Dataframe\n",
        "Miss\n",
        "print(sample.head())\n",
        "print(Original.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MipTRh7btLu1",
        "outputId": "0b516a9b-5a8e-4d94-e3b7-96222066d27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id    Age  Sex  Trt  Error     BP0     BP1     BP2\n",
            "0   9939  50.97    1    0  -1.12  152.78  148.25  149.36\n",
            "1  33905  52.09    0    1  -2.43  145.69  146.69  140.55\n",
            "2   7420  46.86    0    1   1.89  144.52  143.81  139.30\n",
            "3  13429  53.73    1    0   2.20  162.06  158.48  156.92\n",
            "4  65089  48.39    0    0  -1.43  141.41     NaN     NaN\n",
            "   id  Age  Sex  Trt  Error  BP0  BP1  BP2  index\n",
            "0   1   52    1    0     -2  157  155  151      1\n",
            "1   2   53    1    0      0  162  159  160      2\n",
            "2   3   49    0    0      2  149  145  144      3\n",
            "3   4   47    1    0     -3  140  137  134      4\n",
            "4   5   53    1    0      1  160  158  155      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for missing data\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Corruptor:\n",
        "\n",
        "    def __init__(self, data, dtype=np.float64):\n",
        "        self.dtype = data.dtype\n",
        "        self.shape = np.shape(data)\n",
        "        self.data = data.astype(dtype)\n",
        "\n",
        "    def mcar(self):\n",
        "        \"\"\" Overwrites values with MCAR placed NaN's \"\"\"\n",
        "        data_1d = self.flatten()\n",
        "        n_total = len(data_1d)\n",
        "        nan_x = np.random.choice(range(n_total),\n",
        "                                  size=int(n_total*0.3),\n",
        "                                  replace=False)\n",
        "        for x_i in nan_x:\n",
        "            data_1d[x_i] = np.nan\n",
        "        output = data_1d.reshape(self.shape)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "UhrYkf7sHWYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corruptor1:\n",
        "\n",
        "    def __init__(self, data, dtype=np.float64):\n",
        "        self.dtype = data.dtype\n",
        "        self.shape = np.shape(data)\n",
        "        self.data = data.astype(dtype)\n",
        "\n",
        "    def mcar1(self):\n",
        "        \"\"\" Overwrites values with MCAR placed NaN's \"\"\"\n",
        "        data_1d1 = self.flatten()\n",
        "        n_total = len(data_1d1)\n",
        "        nan_x1 = np.random.choice(range(n_total),\n",
        "                                  size=int(n_total*0.5),\n",
        "                                  replace=False)\n",
        "\n",
        "        for x_i1 in nan_x1:\n",
        "            data_1d1[x_i1] = np.nan\n",
        "        output1 = data_1d1.reshape(self.shape)\n",
        "        return output1\n"
      ],
      "metadata": {
        "id": "t7RR9bIrOhQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9DvzPLtYOg8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#need to check why corruptor1 is not working\n",
        "sp=Original.sample(n=200)\n",
        "sc1=np.array([sp['BP2']])\n",
        "sc1=sc1.astype(float)\n",
        "print(sc1)\n",
        "m1=Corruptor1.mcar1(sc1)\n",
        "m1=m1.flatten()\n",
        "print(m1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nTUyNPNcUX",
        "outputId": "1dbe5f4c-b662-4b46-d7d8-8ecc09d80ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[150. 141. 150. 152. 147. 148. 147. 151. 140. 148. 159. 137. 159. 151.\n",
            "  158. 141. 136. 144. 141. 146. 133. 153. 166. 153. 157. 138. 159. 132.\n",
            "  147. 156. 134. 142. 133. 173. 141. 150. 138. 138. 142. 159. 157. 150.\n",
            "  142. 152. 142. 150. 138. 142. 140. 144. 143. 145. 146. 145. 165. 135.\n",
            "  142. 147. 167. 147. 144. 144. 137. 156. 136. 151. 149. 150. 139. 160.\n",
            "  138. 167. 144. 154. 142. 161. 146. 144. 149. 175. 154. 155. 153. 137.\n",
            "  172. 165. 156. 139. 140. 149. 173. 139. 156. 160. 149. 135. 144. 138.\n",
            "  183. 142. 155. 146. 164. 147. 144. 157. 166. 152. 156. 146. 141. 152.\n",
            "  149. 162. 171. 143. 184. 165. 140. 147. 156. 133. 135. 141. 134. 149.\n",
            "  149. 145. 144. 164. 137. 137. 158. 151. 171. 148. 150. 157. 142. 141.\n",
            "  147. 141. 158. 171. 150. 177. 135. 146. 139. 161. 165. 159. 160. 146.\n",
            "  150. 146. 139. 169. 155. 147. 166. 142. 163. 136. 151. 154. 141. 137.\n",
            "  145. 132. 141. 143. 155. 133. 157. 168. 146. 182. 144. 149. 152. 145.\n",
            "  138. 154. 164. 167. 148. 147. 147. 138. 149. 149. 165. 142. 158. 155.\n",
            "  172. 156. 138. 155.]]\n",
            "[150. 141.  nan 152.  nan  nan 147. 151.  nan 148. 159. 137.  nan  nan\n",
            "  nan  nan  nan 144. 141.  nan 133. 153. 166. 153.  nan 138. 159. 132.\n",
            " 147.  nan 134. 142. 133.  nan 141. 150. 138. 138.  nan 159. 157. 150.\n",
            " 142. 152. 142. 150. 138.  nan 140. 144. 143. 145. 146.  nan 165. 135.\n",
            "  nan 147. 167. 147. 144. 144. 137. 156. 136. 151. 149. 150. 139. 160.\n",
            " 138. 167.  nan 154. 142. 161. 146. 144. 149. 175.  nan  nan 153. 137.\n",
            " 172.  nan 156. 139. 140.  nan 173. 139. 156. 160. 149. 135. 144.  nan\n",
            " 183. 142. 155. 146. 164. 147. 144. 157. 166. 152. 156. 146. 141.  nan\n",
            " 149. 162. 171. 143. 184. 165. 140. 147. 156. 133. 135. 141.  nan 149.\n",
            " 149.  nan  nan 164. 137. 137.  nan 151. 171. 148. 150. 157. 142. 141.\n",
            "  nan 141. 158.  nan  nan 177. 135. 146. 139. 161. 165.  nan  nan 146.\n",
            " 150. 146. 139. 169. 155. 147. 166.  nan  nan 136. 151. 154. 141. 137.\n",
            " 145. 132. 141. 143. 155. 133. 157. 168.  nan 182. 144. 149.  nan 145.\n",
            " 138. 154. 164. 167. 148. 147. 147. 138.  nan 149.  nan 142.  nan 155.\n",
            " 172. 156. 138. 155.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating code to find bias\n",
        "def bias(X,Y):\n",
        "    diff=Y-X\n",
        "    m=diff.mean()\n",
        "    return m"
      ],
      "metadata": {
        "id": "Gw7139r9F7YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating RMSE\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "sfLDTObGF8W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating for loop to find everything\n",
        "#empyt\n",
        "#b1 = np.zeros((100, 1))\n",
        "#b2=np.zeros((100, 1))\n",
        "#impb1=np.zeros((100, 1))\n",
        "#impb2=np.zeros((100, 1))\n",
        "\n",
        "imputer=KNNImputer(n_neighbors=3,weights='uniform',metric='nan_euclidean')\n",
        "nan=float(\"NaN\")\n",
        "\n",
        "#results=np.zeros((100,4))\n",
        "\n",
        "def thesis(X):\n",
        "  results=np.zeros((500,4))\n",
        "  num_test=results.shape[0]\n",
        "  num_train=results.shape[1]\n",
        "  for i in range(num_test):\n",
        "    #for j in range(4):\n",
        "      #Creating random sample of 100\n",
        "      sp=Original.sample(n=1000)\n",
        "      #print(\"Original\", sp)\n",
        "      #removing the columns I don't need and creating missing data:\n",
        "      sc=np.array([sp['BP1']])\n",
        "      sc=sc.astype(float)\n",
        "      sc1=np.array([sp['BP2']])\n",
        "      sc1=sc1.astype(float)\n",
        "      m=Corruptor.mcar(sc)\n",
        "      m=m.flatten()\n",
        "      m1=Corruptor1.mcar1(sc1)\n",
        "      m1=m1.flatten()\n",
        "      #print(m1)\n",
        "      #Need to combine these into one data frame with BP0,m,m1\n",
        "      sd=np.array(sp['BP0'])\n",
        "      sd=sd.astype(float)\n",
        "      #Combined as an array\n",
        "      arr = np.column_stack((sd, m,m1))\n",
        "      #Combined as a dataframe\n",
        "      imp=pd.DataFrame({'BP0':arr[:,0],'BP1':arr[:,1],'BP2':arr[:,2]})\n",
        "      #print(imp)\n",
        "      #Now try the imputer on the data frame of BP0 - BP1\n",
        "      sp_imputed=np.round(imputer.fit_transform(imp),8)\n",
        "      #print(sp_imputed.shape)\n",
        "      sp_imputed=pd.DataFrame({'BP0':sp_imputed[:,0],'BP1':sp_imputed[:,1],'BP2':sp_imputed[:,2]})\n",
        "      #print(\"imputed\", sp_imputed)\n",
        "\n",
        "       #Find the  bias\n",
        "      b1=bias(sp['BP1'],sp_imputed['BP1'])\n",
        "      b2=bias(sp['BP2'],sp_imputed['BP2'])\n",
        "      type(b1)==float\n",
        "      type(b2)==float\n",
        "      b1=round(b1,4)\n",
        "      b2=round(b2,4)\n",
        "      #print(b1)\n",
        "      #print(b2)\n",
        "      #Find RMSE\n",
        "      r1 = mean_squared_error(sp['BP1'],sp_imputed['BP1'], squared=False)\n",
        "      type(r1)==float\n",
        "      r1=round(r1,4)\n",
        "      r2 = mean_squared_error(sp['BP2'],sp_imputed['BP2'], squared=False)\n",
        "      type(r2)==float\n",
        "      r2=round(r2,4)\n",
        "      #Storing Results\n",
        "      results[i,0]=b1\n",
        "      results[i,1]=b2\n",
        "      results[i,2]=r1\n",
        "      results[i,3]=r2\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "id": "rsiighcVc4ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=thesis(Original)\n",
        "#print(Y.shape)\n",
        "#print(Y)"
      ],
      "metadata": {
        "id": "s92IEFLrjST5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impdata=pd.DataFrame({'BP1_Bias':Y[:,0],'BP2_Bias':Y[:,1], 'BP1_RMSE':Y[:,2], 'BP2_RMSE':Y[:,3] })\n",
        "#print(impdata.head())"
      ],
      "metadata": {
        "id": "OMuPWgC-GLqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1=impdata.fillna(0)\n",
        "#print(final1)\n",
        "final2=final1.mean()\n",
        "print(final2.round(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duHrHNZTGU22",
        "outputId": "d66b7d83-94f9-4571-c300-eb88bcb15b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BP1_Bias   -0.1136\n",
            "BP2_Bias   -0.2192\n",
            "BP1_RMSE    1.3049\n",
            "BP2_RMSE    1.7297\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#creating sample of 500\n",
        "#Creating random sample of 100\n",
        "sp=Original.sample(n=100)\n",
        "\n",
        "#print(sp.head())\n",
        "#removing the columns I don't need and creating missing data:\n",
        "sc=np.array([sp['BP1']])\n",
        "sc=sc.astype(float)\n",
        "sc1=np.array([sp['BP2']])\n",
        "sc1=sc1.astype(float)\n",
        "m=Corruptor.mcar(sc)\n",
        "m=m.flatten()\n",
        "m1=Corruptor.mcar(sc1)\n",
        "m1=m1.flatten()\n",
        "\n",
        "\n",
        "#Need to combine these into one data frame with BP0,m,m1\n",
        "sd=np.array(sp['BP0'])\n",
        "sd=sd.astype(float)\n",
        "#Combined as an array\n",
        "arr = np.column_stack((sd, m,m1))\n",
        "#Combined as a dataframe\n",
        "imp=pd.DataFrame({'BP0':arr[:,0],'BP1':arr[:,1],'BP2':arr[:,2]})\n",
        "\n",
        "print(imp.head)\n",
        "\n"
      ],
      "metadata": {
        "id": "em7KCgT-IWya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating code to handle Missing data\n",
        "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer=KNNImputer(n_neighbors=3,weights='uniform',metric='nan_euclidean')\n",
        "imputer1=KNNImputer(n_neighbors=3,weights='distance',metric='nan_euclidean')\n",
        "nan=float(\"NaN\")\n"
      ],
      "metadata": {
        "id": "6R2DNX0jjruj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now try the imputer on the data frame of BP0 - BP1\n",
        "sp_imputed=np.round(imputer.fit_transform(imp),2)\n",
        "#print(sp_imputed)\n",
        "\n",
        "sp_imputed=pd.DataFrame({'BP0':sp_imputed[:,0],'BP1':sp_imputed[:,1],'BP2':sp_imputed[:,2]})\n",
        "#sp_imputed1=np.round(imputer.fit_transform(imp),2)\n",
        "print(sp_imputed.head)\n",
        "print(sp_imputed.var())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI4vgOOrjYik",
        "outputId": "e78f0e5b-0716-4707-9ae8-a19818362e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of        BP0     BP1     BP2\n",
            "0    171.0  166.00  165.00\n",
            "1    160.0  164.00  154.00\n",
            "2    146.0  145.00  140.00\n",
            "3    146.0  143.00  145.00\n",
            "4    165.0  162.00  161.33\n",
            "..     ...     ...     ...\n",
            "495  145.0  141.33  141.00\n",
            "496  140.0  137.00  135.00\n",
            "497  144.0  142.00  140.00\n",
            "498  177.0  176.33  174.00\n",
            "499  161.0  160.00  156.00\n",
            "\n",
            "[500 rows x 3 columns]>\n",
            "BP0    112.430846\n",
            "BP1    111.619025\n",
            "BP2    115.482222\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating RMSE and Bias\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#converting imputed datat into a dataframe\n",
        "#sp1=pd.DataFrame('BP0':sp_imputed[:,0],'BP1':sp_imputed[:,0])\n",
        "\n",
        "#sp1=np.array(sp_imputed['BP1'])\n",
        "r1 = mean_squared_error(sp['BP1'], sp_imputed['BP1'], squared=False)\n",
        "print(r1)\n",
        "r2=mean_squared_error(sp['BP2'], sp_imputed['BP2'], squared=False)\n",
        "print(r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXZxYcvHpUBR",
        "outputId": "c44247ab-f437-4d47-b6bc-ad2faa1456ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6037444823764443\n",
            "0.7151041882131581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting dataframe to nparray\n",
        "M=Miss.to_numpy()\n",
        "print(M.shape)\n",
        "\n",
        "#Transposing Missing data array\n",
        "M=M.reshape(M.shape[1],-1)\n",
        "print(M.shape)\n",
        "\n",
        "#Creating numpy array to check distance with\n",
        "list1=np.random.rand(1,100)\n",
        "print(list1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezdk-9_KuQbk",
        "outputId": "7484b36c-d617-462c-cf77-3e3e914f0775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1)\n",
            "(1, 100)\n",
            "(1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating set for KNN Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Keep last row of each subject\n",
        "subjects = sample.groupby('id').last().reset_index()\n",
        "\n",
        "# Split this data stratifying by `trt`\n",
        "x_train, x_test, y_train, y_test = train_test_split(subjects['BP1'],subjects[\"Trt\"], train_size=0.8, test_size=0.2, stratify=subjects['Trt'],random_state=42)"
      ],
      "metadata": {
        "id": "nEpvEge48cW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(x_train.shape)\n",
        "print(y_test.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "#converting x_test and x_train to numpy array\n",
        "x_train=np.array([x_train])\n",
        "x_test=np.array([x_test])\n",
        "\n",
        "print(x_train)\n",
        "print(x_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY3uv9zq_ebk",
        "outputId": "2a37b05a-60ab-40bb-89db-35678195b06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80,)\n",
            "(80,)\n",
            "(20,)\n",
            "(20,)\n",
            "[[158.07 158.93 148.69 161.02 145.15 147.41 143.35 153.52 162.42 163.37\n",
            "  158.48 151.91 160.87 142.77 150.55 146.4  165.93 166.57    nan 150.67\n",
            "  158.5  153.22 137.54    nan 138.5  135.29 142.57 146.06 161.13 150.73\n",
            "  144.55    nan 161.4  145.41 178.57    nan 142.5  141.88 167.73 151.24\n",
            "  160.26 150.91 155.99 140.88 149.99 170.8  158.28 142.83 146.65 149.9\n",
            "     nan 173.46 148.79 150.   148.22 138.8  140.41 148.97 146.69 142.67\n",
            "  139.95 144.31    nan 143.81    nan 141.84 156.55    nan    nan 177.28\n",
            "  148.76 160.37 145.38 148.25    nan 168.23 159.71 162.39 148.45 151.48]]\n",
            "[[165.9  155.05 158.67 142.8  149.07 167.62 163.59 140.5  144.6  177.93\n",
            "  165.68 145.88 161.46 150.52 157.65 146.59 147.66 144.85 149.97 149.71]]\n",
            "(1, 80)\n",
            "(1, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Need for P-adic distances\n",
        "\n",
        "#Checking Prime Number\n",
        "def create_seive(n):\n",
        "    '''Creates a list of primality of integers up to n.\n",
        "    n Should be an positive, if not integer will round down to closest int '''\n",
        "    try:\n",
        "        n = int(n)\n",
        "    except ValueError:\n",
        "        raise ValueError\n",
        "\n",
        "    assert n > 0\n",
        "\n",
        "    my_list = [True] * (n+1)\n",
        "    my_list[0] = False\n",
        "    my_list[1] = False\n",
        "\n",
        "    for j in range(2, n//2 + 1):\n",
        "        for i in range(j, n+1, j):\n",
        "            if i != j:\n",
        "                my_list[i] = False\n",
        "\n",
        "    return my_list\n",
        "\n",
        "max_prime = 1000\n",
        "is_prime_list = create_seive(max_prime)\n",
        "\n",
        "def prime_check(p):\n",
        "    '''Given a p check that it is a prime number between 0 and 1000'''\n",
        "    try:\n",
        "        p = int(p)\n",
        "        is_prime = is_prime_list[p]\n",
        "    except ValueError:\n",
        "        return False\n",
        "    return is_prime\n",
        "\n",
        "def validate_prime(p):\n",
        "    if not prime_check(p):\n",
        "        raise ValueError\n",
        "\n",
        "def find_p_ord_int(a, p):\n",
        "    '''Calculate the ordinal of a, an int, with respect to p a prime.'''\n",
        "    if a == 0:\n",
        "        validate_prime(p)\n",
        "        return float(\"inf\")\n",
        "    elif isinstance(a, Fraction):\n",
        "        raise 'Error a is a fraction must be an int.'\n",
        "    try:\n",
        "        a = int(a)\n",
        "    except ValueError:\n",
        "        raise ValueError\n",
        "    validate_prime(p)\n",
        "    c = 0\n",
        "    while a % (p**c) == 0:\n",
        "        c += 1\n",
        "    return c - 1\n",
        "\n",
        "def find_p_ord(a, p):\n",
        "    '''Calculate the ordinal of a rational number a\n",
        "        with respect to a prime p.'''\n",
        "    if a == 0:\n",
        "        validate_prime(p)\n",
        "        return float(0)\n",
        "    elif isinstance(a, int):\n",
        "        return find_p_ord_int(a, p)\n",
        "    else:\n",
        "        try:\n",
        "            frac = Fraction(str(a)).limit_denominator()\n",
        "        except ValueError:\n",
        "            raise ValueError\n",
        "        numerator = find_p_ord_int(frac.numerator, p)\n",
        "        denominator = find_p_ord_int(frac.denominator, p)\n",
        "        return numerator - denominator\n",
        "\n",
        "#    '''Calculate the p-adic norm of a, a rational number,with repect to p a prime.''\n",
        "def p_norm(self, p):\n",
        "    vect=self.shape[0]\n",
        "    values=np.zeros(vect)\n",
        "\n",
        "    for k in range(vect):\n",
        "        a=self[k].flatten().max()\n",
        "        values[k]=1/(p**find_p_ord(a, p))\n",
        "    return values"
      ],
      "metadata": {
        "id": "mOJ11Fp7wAw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First creating euclidean code  for self=(1,3) and X=(1,3)\n",
        "def na_euclidean(X, Y):\n",
        "    num_test = X.shape[0]\n",
        "    num_train = Y.shape[0]\n",
        "    dists = np.zeros((num_test, num_train))\n",
        "    dists=nan_euclidean_distances(X,Y)\n",
        "    return dists"
      ],
      "metadata": {
        "id": "_3Dd51N5xwhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u=na_euclidean(list1,M)\n",
        "print(u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbzSOUgmxx7K",
        "outputId": "5e77e9b8-6ab0-4cfd-ec44-06372695cdd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1516.49588097]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seconding creating hammington distance for missing data\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "import itertools\n",
        "import warnings\n",
        "from functools import partial\n",
        "from numbers import Integral, Real\n",
        "\n",
        "import numpy as np\n",
        "from joblib import effective_n_jobs\n",
        "from scipy.sparse import csr_matrix, issparse\n",
        "from scipy.spatial import distance\n",
        "\n",
        "from sklearn import config_context\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.utils import (\n",
        "    check_array,\n",
        "    gen_batches,\n",
        "    gen_even_slices,\n",
        "    get_chunk_n_rows,\n",
        "    is_scalar_nan,\n",
        ")\n",
        "from sklearn.utils._mask import _get_mask\n",
        "\n",
        "from sklearn.utils.extmath import row_norms, safe_sparse_dot\n",
        "\n",
        "from sklearn.utils.parallel import Parallel, delayed\n",
        "from sklearn.utils.validation import _num_samples, check_non_negative\n",
        "\n",
        "#need this code\n",
        "def _return_float_dtype(X, Y):\n",
        "    \"\"\"\n",
        "    1. If dtype of X and Y is float32, then dtype float32 is returned.\n",
        "    2. Else dtype float is returned.\n",
        "    \"\"\"\n",
        "    if not issparse(X) and not isinstance(X, np.ndarray):\n",
        "        X = np.asarray(X)\n",
        "\n",
        "    if Y is None:\n",
        "        Y_dtype = X.dtype\n",
        "    elif not issparse(Y) and not isinstance(Y, np.ndarray):\n",
        "        Y = np.asarray(Y)\n",
        "        Y_dtype = Y.dtype\n",
        "    else:\n",
        "        Y_dtype = Y.dtype\n",
        "\n",
        "    if X.dtype == Y_dtype == np.float32:\n",
        "        dtype = np.float32\n",
        "    else:\n",
        "        dtype = float\n",
        "\n",
        "    return X, Y, dtype\n",
        "\n",
        "\n",
        "#Need this code\n",
        "def check_pairwise_arrays(\n",
        "    X,\n",
        "    Y,\n",
        "    *,\n",
        "    precomputed=False,\n",
        "    dtype=None,\n",
        "    accept_sparse=\"csr\",\n",
        "    force_all_finite=True,\n",
        "    copy=False,):\n",
        "\n",
        "    X, Y, dtype_float = _return_float_dtype(X, Y)\n",
        "\n",
        "    estimator = \"check_pairwise_arrays\"\n",
        "    if dtype is None:\n",
        "        dtype = dtype_float\n",
        "\n",
        "    if Y is X or Y is None:\n",
        "        X = Y = check_array(\n",
        "            X,\n",
        "            accept_sparse=accept_sparse,\n",
        "            dtype=dtype,\n",
        "            copy=copy,\n",
        "            force_all_finite=force_all_finite,\n",
        "            estimator=estimator,\n",
        "        )\n",
        "    else:\n",
        "        X = check_array(\n",
        "            X,\n",
        "            accept_sparse=accept_sparse,\n",
        "            dtype=dtype,\n",
        "            copy=copy,\n",
        "            force_all_finite=force_all_finite,\n",
        "            estimator=estimator,\n",
        "        )\n",
        "        Y = check_array(\n",
        "            Y,\n",
        "            accept_sparse=accept_sparse,\n",
        "            dtype=dtype,\n",
        "            copy=copy,\n",
        "            force_all_finite=force_all_finite,\n",
        "            estimator=estimator,\n",
        "        )\n",
        "\n",
        "    if precomputed:\n",
        "        if X.shape[1] != Y.shape[0]:\n",
        "            raise ValueError(\n",
        "                \"Precomputed metric requires shape \"\n",
        "                \"(n_queries, n_indexed). Got (%d, %d) \"\n",
        "                \"for %d indexed.\" % (X.shape[0], X.shape[1], Y.shape[0])\n",
        "            )\n",
        "    elif X.shape[1] != Y.shape[1]:\n",
        "        raise ValueError(\n",
        "            \"Incompatible dimension for X and Y matrices: \"\n",
        "            \"X.shape[1] == %d while Y.shape[1] == %d\" % (X.shape[1], Y.shape[1])\n",
        "        )\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def nan_manhattan_distances(X, Y=None, missing_values=np.nan, copy=True):\n",
        "\n",
        "    force_all_finite = \"allow-nan\" if is_scalar_nan(missing_values) else True\n",
        "    X, Y = check_pairwise_arrays(\n",
        "        X, Y, accept_sparse=False, force_all_finite=force_all_finite, copy=copy)\n",
        "\n",
        "    # Get missing mask for X\n",
        "    missing_X = _get_mask(X, missing_values)\n",
        "\n",
        "    # Get missing mask for Y\n",
        "    missing_Y = missing_X if Y is X else _get_mask(Y, missing_values)\n",
        "\n",
        "    # set missing values to zero\n",
        "    X[missing_X] = 0\n",
        "    Y[missing_Y] = 0\n",
        "\n",
        "    X, Y = check_pairwise_arrays(X, Y)\n",
        "\n",
        "    distances = manhattan_distances(X, Y)\n",
        "\n",
        "    return distances"
      ],
      "metadata": {
        "id": "vZ_173Y_x7sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if nan_manhattan_distances code works\n",
        "h=nan_manhattan_distances(list1,M)\n",
        "print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0prefvPryIf6",
        "outputId": "bebf8838-da86-4342-b068-2e76de5947c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12117.48587576]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now creating p-adic distances with missing data using manhattan distances\n",
        "def nan_padic_distances(X, Y,p):\n",
        "    num_test = X.shape[0]\n",
        "    num_train = Y.shape[0]\n",
        "    dists = np.zeros((num_test, num_train))\n",
        "    dist=nan_manhattan_distances(X,Y)\n",
        "    dists = p_norm(dist,p)\n",
        "    dists=dists.reshape(-1,1)\n",
        "    return dists\n"
      ],
      "metadata": {
        "id": "WKpgUQnEyTvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=nan_padic_distances(list1,M,5)\n",
        "print(t)"
      ],
      "metadata": {
        "id": "vJ6UKkHDyWQi",
        "outputId": "b1536dde-3c64-4588-804b-40497654e0d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of distance metrics to try\n",
        "distance_metrics = ['exc_dist','nan_euclidean','nan_manhattan']\n",
        "prime_numbers = [2,3,5,7,11,13,17,19,23,29]\n",
        "n_neighbors = 3\n",
        "\n",
        "\n",
        "#print(x_train.shape)\n",
        "#print(y_train.shape)"
      ],
      "metadata": {
        "id": "tDdLWQt97kjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using KNN Imputer to fill in missing data\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer=KNNImputer(n_neighbors=3,weights='uniform',metric='nan_euclidean')\n",
        "\n",
        "print(x_train)\n",
        "print(x_test)\n",
        "x_train=imputer.fit_transform(x_train)\n",
        "x_test=imputer.fit_transform(x_test)\n",
        "print(x_train)\n",
        "print(x_test)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(x_train.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eX6IgQa6Uy2",
        "outputId": "0d52ee80-710f-45d3-bd01-f0c1d12bc58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[158.07 158.93 148.69 161.02 145.15 147.41 143.35 153.52 162.42 163.37\n",
            "  158.48 151.91 160.87 142.77 150.55 146.4  165.93 166.57 150.67 158.5\n",
            "  153.22 137.54 138.5  135.29 142.57 146.06 161.13 150.73 144.55 161.4\n",
            "  145.41 178.57 142.5  141.88 167.73 151.24 160.26 150.91 155.99 140.88\n",
            "  149.99 170.8  158.28 142.83 146.65 149.9  173.46 148.79 150.   148.22\n",
            "  138.8  140.41 148.97 146.69 142.67 139.95 144.31 143.81 141.84 156.55\n",
            "  177.28 148.76 160.37 145.38 148.25 168.23 159.71 162.39 148.45 151.48]]\n",
            "[[165.9  155.05 158.67 142.8  149.07 167.62 163.59 140.5  144.6  177.93\n",
            "  165.68 145.88 161.46 150.52 157.65 146.59 147.66 144.85 149.97 149.71]]\n",
            "[[158.07 158.93 148.69 161.02 145.15 147.41 143.35 153.52 162.42 163.37\n",
            "  158.48 151.91 160.87 142.77 150.55 146.4  165.93 166.57 150.67 158.5\n",
            "  153.22 137.54 138.5  135.29 142.57 146.06 161.13 150.73 144.55 161.4\n",
            "  145.41 178.57 142.5  141.88 167.73 151.24 160.26 150.91 155.99 140.88\n",
            "  149.99 170.8  158.28 142.83 146.65 149.9  173.46 148.79 150.   148.22\n",
            "  138.8  140.41 148.97 146.69 142.67 139.95 144.31 143.81 141.84 156.55\n",
            "  177.28 148.76 160.37 145.38 148.25 168.23 159.71 162.39 148.45 151.48]]\n",
            "[[165.9  155.05 158.67 142.8  149.07 167.62 163.59 140.5  144.6  177.93\n",
            "  165.68 145.88 161.46 150.52 157.65 146.59 147.66 144.85 149.97 149.71]]\n",
            "(80,)\n",
            "(1, 70)\n",
            "       id    Age  Sex  Trt  Error     BP0     BP1     BP2\n",
            "0    9939  50.97    1    0  -1.12  152.78  148.25  149.36\n",
            "1   33905  52.09    0    1  -2.43  145.69  146.69  140.55\n",
            "2    7420  46.86    0    1   1.89  144.52  143.81  139.30\n",
            "3   13429  53.73    1    0   2.20  162.06  158.48  156.92\n",
            "4   65089  48.39    0    0  -1.43  141.41     NaN     NaN\n",
            "..    ...    ...  ...  ...    ...     ...     ...     ...\n",
            "95  17255  48.47    1    0   2.02  146.70  144.60  142.53\n",
            "96  10958  51.21    0    0  -3.69  147.88  142.50  143.44\n",
            "97  23581  57.45    0    1   0.78  171.22  165.90  168.85\n",
            "98  64892  52.11    1    1   2.32  156.44  155.99  150.91\n",
            "99  13286  49.15    0    1   1.77  150.28  147.66  148.13\n",
            "\n",
            "[100 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now try the imputer on the data frame of BP0 - BP1\n",
        "#print(sample)\n",
        "sample1=pd.DataFrame(sample[['BP0','BP1','BP2']])\n",
        "print(sample1)\n",
        "sample_imputed=np.round(imputer.fit_transform(sample1),2)\n",
        "print(sample_imputed)\n",
        "\n",
        "#Check Bias\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNnNFtSYnILm",
        "outputId": "8795de93-4361-46fa-95e5-9f422711ac09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       BP0     BP1     BP2\n",
            "0   152.78  148.25  149.36\n",
            "1   145.69  146.69  140.55\n",
            "2   144.52  143.81  139.30\n",
            "3   162.06  158.48  156.92\n",
            "4   141.41     NaN     NaN\n",
            "..     ...     ...     ...\n",
            "95  146.70  144.60  142.53\n",
            "96  147.88  142.50  143.44\n",
            "97  171.22  165.90  168.85\n",
            "98  156.44  155.99  150.91\n",
            "99  150.28  147.66  148.13\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "[[152.78 148.25 149.36]\n",
            " [145.69 146.69 140.55]\n",
            " [144.52 143.81 139.3 ]\n",
            " [162.06 158.48 156.92]\n",
            " [141.41 138.24 140.6 ]\n",
            " [148.49 148.76 143.8 ]\n",
            " [162.1  161.46 155.78]\n",
            " [143.69 138.5  141.81]\n",
            " [148.38 146.65 141.03]\n",
            " [145.84 142.83 143.87]\n",
            " [139.32 138.24 140.6 ]\n",
            " [148.69 148.45 140.39]\n",
            " [149.74 146.06 145.77]\n",
            " [144.98 144.55 141.74]\n",
            " [141.98 135.29 141.81]\n",
            " [143.77 140.41 141.28]\n",
            " [168.32 163.59 165.42]\n",
            " [142.42 139.95 141.28]\n",
            " [150.63 150.73 144.49]\n",
            " [160.22 158.67 157.56]\n",
            " [157.19 160.26 150.18]\n",
            " [156.9  150.67 151.73]\n",
            " [156.18 153.22 152.47]\n",
            " [151.95 149.99 148.05]\n",
            " [163.38 161.4  161.35]\n",
            " [165.86 160.37 160.78]\n",
            " [171.07 168.23 163.38]\n",
            " [141.62 141.88 141.31]\n",
            " [159.41 157.65 153.93]\n",
            " [155.72 150.52 150.56]\n",
            " [165.88 166.57 161.22]\n",
            " [167.79 165.68 167.88]\n",
            " [146.57 141.84 140.81]\n",
            " [160.37 161.13 154.55]\n",
            " [158.18 156.55 159.5 ]\n",
            " [144.26 142.57 140.75]\n",
            " [165.12 162.42 161.51]\n",
            " [148.01 145.41 139.05]\n",
            " [154.14 148.69 148.89]\n",
            " [146.99 142.8  143.49]\n",
            " [175.54 178.57 173.42]\n",
            " [170.2  167.73 167.34]\n",
            " [151.11 148.22 144.19]\n",
            " [149.91 149.07 143.77]\n",
            " [162.73 159.71 159.74]\n",
            " [142.46 138.8  141.28]\n",
            " [149.3  145.15 142.35]\n",
            " [179.95 177.93 179.24]\n",
            " [149.4  145.88 141.83]\n",
            " [172.37 170.8  166.4 ]\n",
            " [160.04 161.02 153.31]\n",
            " [148.67 149.9  145.63]\n",
            " [150.97 149.71 146.91]\n",
            " [159.98 158.5  153.82]\n",
            " [161.13 158.07 155.14]\n",
            " [153.29 151.48 148.53]\n",
            " [151.22 150.55 146.56]\n",
            " [154.97 151.24 152.28]\n",
            " [140.33 138.24 140.6 ]\n",
            " [155.25 149.97 149.2 ]\n",
            " [139.34 138.24 140.6 ]\n",
            " [146.4  144.85 140.25]\n",
            " [164.67 162.39 161.45]\n",
            " [176.32 173.46 173.67]\n",
            " [158.79 160.87 154.39]\n",
            " [165.61 165.93 160.7 ]\n",
            " [142.2  137.54 141.81]\n",
            " [145.86 144.31 139.27]\n",
            " [180.86 177.28 178.58]\n",
            " [169.21 167.62 167.59]\n",
            " [142.27 140.88 141.31]\n",
            " [150.47 150.91 147.21]\n",
            " [140.25 138.24 140.6 ]\n",
            " [145.62 142.77 139.22]\n",
            " [153.73 155.05 149.95]\n",
            " [162.05 158.28 158.46]\n",
            " [147.97 145.38 139.36]\n",
            " [153.95 153.52 150.53]\n",
            " [150.64 148.79 146.24]\n",
            " [161.26 158.93 154.78]\n",
            " [140.28 138.24 140.6 ]\n",
            " [152.35 151.91 150.24]\n",
            " [139.53 138.24 140.6 ]\n",
            " [140.74 138.24 140.6 ]\n",
            " [153.73 150.   148.81]\n",
            " [145.22 142.67 143.88]\n",
            " [167.41 163.37 166.05]\n",
            " [141.24 138.24 140.6 ]\n",
            " [149.11 146.59 143.66]\n",
            " [146.4  147.41 143.74]\n",
            " [142.77 143.35 140.6 ]\n",
            " [141.6  138.24 140.6 ]\n",
            " [150.61 148.97 145.29]\n",
            " [149.66 146.4  144.45]\n",
            " [143.82 140.5  141.28]\n",
            " [146.7  144.6  142.53]\n",
            " [147.88 142.5  143.44]\n",
            " [171.22 165.9  168.85]\n",
            " [156.44 155.99 150.91]\n",
            " [150.28 147.66 148.13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating RMSE and Bias\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n"
      ],
      "metadata": {
        "id": "8vAlElRipG_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byEwOOBN5hsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}